\section{Dynamic games}

\subsection{Escape Game Example}

The escape game is a zero-sum feedback game where Alice tries to
reach the safe zone over $K$ stages without being intercepted by Eve.
Alice moves up, middle, or down; Eve blocks one row. The game tree
grows exponentially (e.g., for 3 actions, order of $1 + 3 + 9 + \dots
+ 3^{K-1}$ nodes), requiring backward induction on the full tree,
which is computationally intractable for large $K$ (e.g., $5 \times
10^9$ LPs for $K=20$).

% \begin{center}
%   \begin{tikzpicture}[
%
% % WARN: topics/linear-quadratic-games.tex|33 error| Package pgfkeys
% Error: I do not know the key '/tikz/middle' and I am going to
% ignore it. Perhaps you misspelled it.
%       level distance=1.5cm,
%       sibling distance=2cm,
%       edge from parent/.style={draw, -latex},
%       every node/.style={align=center, draw, minimum size=1cm}
%     ]
%     \node {Start (C)}
%     child { node {U}
%       child { node {U} edge from parent node[left] {U} }
%       child { node {C} edge from parent node[right] {C} }
%     }
%     child { node {C}
%       child { node {U} edge from parent node[left] {U} }
%       child { node {C} edge from parent node[middle] {C} }
%       child { node {D} edge from parent node[right] {D} }
%     }
%     child { node {D}
%       child { node {C} edge from parent node[left] {C} }
%       child { node {D} edge from parent node[right] {D} }
%     };
%   \end{tikzpicture}
% \end{center}

\subsubsection{Taming Complexity in Dynamic Games}

Model as a state-based \emph{loop model} instead of tree:

states $X = \{U, C, D, L\}$ (up, center, down, lost)

actions $U(x_k)$ and $V(x_k)$ depend on state.

Dynamics:
$x_{k+1} = f(x_k, u_k, v_k) = L$ if $u_k = v_k$ or $x_k = L$,
else $x_{k+1} = u_k$.

Stage cost $g_k = 0$ for $k < K$, $g_K = +1$ if lost (Eve wins), $-1$
if safe (Alice wins).

Backward induction on states reduces to $4 \times K$ LPs (scalable).
Value function $V_k(x_k)$ computed recursively: $V_K(x_K)$ from
last-stage NE, then
\[
  V_{k-1}(x_{k-1}) = \min_\gamma \max_\sigma [g_{k-1} + V_k(f(\cdot))]
\]

For infinite $K$, $V_1(C) \to 0$ (Alice's escape probability
approaches 0.5 numerically, as Eve can always adapt).

This state-based approach is preferable to tree induction for large
$K$ or infinite horizons, enabling efficient computation in control
applications.

\subsubsection{From Tree to Loop Model}

Tree models capture full history,
loop models abstract to Markov states evolving via
$x_{k+1} = f(x_k, u_k, v_k, \dots)$,
with outcomes $g_k(x_k, u_k, v_k, \dots)$.
Equivalent for feedback games, allowing control interpretations
(strategies as feedback laws).

\subsection{Non-Zero-Sum Dynamic Games}

In multi-player settings (e.g., three-truck platoon),\\
players minimize individual costs:
\[
  J_i = \sum_k g_{i,k}(x_k, u_{1,k}, \dots, u_{n,k}) + g_{i,K}(x_K)
\]
Subgame-perfect Nash Equilibrium (SPNE): strategies form NE in every subgame.

SPNE corresponds to optimal feedback laws in control theory.

\subsubsection{Backward Induction}

For finite-horizon games, compute SPNE via backward induction on states:

\textbf{At stage} $K$ -
Solve static game for each $x_K$:\\
find NE strategies
$\gamma_K^*(x_K)$, value $V_K(x_K) = g_K + g_\text{terminal}$.

\textbf{Recursively}
at stage $k$ - For each $x_k$:\\
Solve static game with payoff
$g_k(x_k, u_k) + V_{k+1}(f(x_k, u_k))$,
yielding $\gamma_k^*(x_k)$, $V_k(x_k)$.

\examBox{Relevant for exams:}
{Ensures subgame perfection; extends to stochastic
games (with probabilities in dynamics)}

\subsubsection{One-Player Case: Dynamic Programming}

Reduces to Bellman's equation:
\[
  V_k(x_k) = \min_{u_k} [g_k(x_k, u_k) + V_{k+1}(f(x_k, u_k))]
\]
optimal policy $\mu_k^*(x_k)$.

\subsubsection{Two-Player Linear-Quadratic (LQ) Games}

System: $x_{k+1} = A x_k + B_1 u_k + B_2 v_k$.

Costs (non-zero-sum):

$J_1 = \sum_{k=0}^{K-1} (x_k^T Q_1 x_k + u_k^T R_1 u_k + v_k^T
S_1 v_k) + x_K^T P_1 x_K$,

$J_2 = \sum_{k=0}^{K-1} (x_k^T Q_2 x_k + v_k^T R_2 v_k + u_k^T S_2
u_k) + x_K^T P_2 x_K$

Backward induction yields affine feedback:

Assume $V_{1,k}(x) = x^T P_{1,k} x + q_{1,k}^T x + r_{1,k}$, \\
similarly for $V_{2,k}$.

At each step, solve for NE:

$\hat{u}_k = - (R_1 + B_1^T P_{1,k+1} B_1)^{-1} (B_1^T P_{1,k+1} A
x_k + B_1^T P_{1,k+1} B_2 v_k + \dots)$
(coupled)

Results in Riccati-like equations for $P_{i,k}$, enabling unique NE
under positive-definiteness assumptions (convex costs).

\examBox{Exam relevance}
{Mirrors uniqueness in convex games; dynamics like projected
gradient; backward induction in stochastic/epidemic games}

\examBox{Exercises Insights}
{Exercises reinforce backward induction in escape game: Compute mixed
  NE numerically, implement in MATLAB, analyze convergence. Highlights
scalability of state-based methods over tree-based for dynamic/repeated games. }

