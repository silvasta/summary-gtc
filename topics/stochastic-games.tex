\section{Stochastic games}

Stochastic games extend randomized feedback games to model
multi-stage interactions with state-dependent transitions, addressing
intractability for large/infinite horizons by reducing complexity via
state aggregation.

Key objectives:

- Identify limitations of randomized feedback games for large/infinite stages.

- Introduce stochastic games as a tractable subclass.

- Construct state-transition models.

- Differentiate solution methods for finite vs. infinite-stage games.

- (Bonus) Mitigate curse of dimensionality via stochastic population games.

\subsection{Motivating Example: Epidemic Game}

Two players: Alice (initially Not Infected, NI) and Eve (initially Infected, I).
Actions: H (stay home, boredom cost 1 for NI) or O (go out, cost 0
but infection risk).
Eve's illness cost: 3 (base), reduced to 2 if both H (quarantine).
Infection prob. α if both O; recovery prob. β if infected stays H.
States: (NI,I), (I,I) (absorbing if both infected).
Relevant for exams: Model as stochastic game; compute equilibria via
backward induction; analyze parameter-dependent pure Nash (e.g.,
quarantine vs. go out).

\subsubsection{1 Stage (One-Shot Game)}

- Game tree: Simple extensive form with states.

- Not a full feedback game due to randomization.

- Nash: Depends on α, β; e.g., both O if α low.

- Values: Expected costs for each player.

\subsubsection{2 Stages}

- Game tree: Branches based on transitions; solve subgames.

- Feedback game with states reducing info sets from $4^{K-i}$ to $2K$.

- Solve via backward induction: 2 LPs per state (matrix games).

- Equilibria shift with α, β (e.g., cooperation if recovery likely).

\subsubsection{Taming Complexity}

- subgames dependent of states \{(NI,I),(I,I)\}

- from $4^{k-i}$ to $2K$

\subsection{State transition model}

- \textbf{Stages} become \textbf{discrete time}

- \textbf{Behavioral strategies}
(maps from information sets to actions)
become \textbf{feedback policies}
(maps from states to inputs)

- \textbf{Kuhn’s theorem holds}:
NE in behavioral strategies guaranteed to exist

- Can compute (subgame-perfect) behavioral NE
using \textbf{backward induction}

\subsection{Probabilistic state transitions}

- from certainty $1$ to probability $\alpha$

\subsection{Stochastic game}

\begin{sstTitleBox}
  {Basic elements}
  \begin{sstOnlyFrame}
    \begin{itemize}
      \item Stages $k = 0,\dots, K−1$
      \item State space
        $x_k \in \mathcal{X} = \{x^1,\dots, x^\ell\}$,
        initial state $x_0$
      \item Action space
        $u_k \in \mathcal{U} = \{u^1,\dots,u^n\}$,
        $v_k \in \mathcal{V} = \{v^1,\dots,v^m\}$
      \item Stage outcome functions
        $g^{(1)} (x_k , u_k , v_k )$,
        $g^{(2)} (x_k , u_k , v_k )$
      \item State transition probabilities
        $\mathbb{P}(x_{k+1} | x_k , u_k , v_k )$
    \end{itemize}
  \end{sstOnlyFrame}
\end{sstTitleBox}

\subsubsection{Finite-Stage Stochastic Game}

- Strategies: Time-dependent behavioral $\gamma_k^b(x), \sigma_k^b(x)$.

- Payoff: $\mathbb{E}[\sum_{k=0}^{K-1} g | x_0]$.

- Solve: Backward induction over ℓK matrix games per state/time.

\subsubsection{Infinite-Stage Stochastic Game}

- Strategies: Stationary $\gamma^b(x), \sigma^b(x)$

- Payoff: $\mathbb{E}[\sum_{k=0}^\infty \delta^k g | x_0]$ (discounted)

- Solve: Policy iteration
(init strategies; evaluate V via linear system;
improve via best-response games);
or value iteration.

- Theorem: If converges, yields subgame-perfect behavioral NE.

- Warning: Convergence not guaranteed (unlike MDPs); counterexamples exist.

\subsection{Value Functions}

- Finite: $V_k^i(x) =$
expected remaining payoff from k,
separable as stage $g + V_{k+1}$

- Infinite:
$V^i(x) = \mathbb{E}[g^i(x,u,v) + δ V^i(x')]$
solves Bellman Eq.

\subsection{Example: Traffic Game (2 Cars at Crossing)}

- Matrix: $
\begin{pmatrix} (100,100) & (0,1) \\ (1,0) & (v,v)
\end{pmatrix}$

- Actions: Go/Wait; crash if both Go.

- Solve NE via p,q mixtures; value $y^T G z$.

- Extend to stochastic: Add states (e.g., traffic light), transitions.

\subsection{Summary: Stochastic Games}

Stochastic games model state-dependent multi-stage play with
probabilistic transitions. Finite: Backward induction for
subgame-perfect NE. Infinite: Stationary policies, Bellman eq.,
policy/value iteration (convergence not assured).
\examBox{Exam focus}
{Epidemic modeling, parameter-dependent equilibria, backward induction
in finite stages, value functions.}

\section{Bonus Material: Stochastic Population Games}
Curse of dimensionality: State space explodes
($2^N$ for $N$ players, e.g., epidemics).

Assumptions for tractability (mean-field limit, N→∞):
1. Large symmetric anonymous players.
2. Separable states.
3. Payoffs/transitions depend on own state/action + distributions
$(d_k, \pi_k)$.
4. Continuity in distributions.

Theorem:
Exists stationary $(d^*, \pi^*)$ as NE for $\delta$-discounted infinite game.
Reduces dimension to $~p+n$ (independent of N);
e.g., city epidemic from $2^{400k}$ to $~4$ dims.

Uncovered topics: Partial info/Bayesian games, differential games.

